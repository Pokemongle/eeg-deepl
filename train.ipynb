{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089503e4",
   "metadata": {},
   "source": [
    "# Head Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0232fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import MyData  # self-made\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm # View procedure\n",
    "import os\n",
    "import scipy.io\n",
    "from random import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from network_cnn_lstm_4 import MyNetwork\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d854be2",
   "metadata": {},
   "source": [
    "# 1. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521251cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1231, 2400, 10, 11])\n",
      "torch.Size([1353, 2400, 10, 11])\n",
      "torch.Size([1220, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "exper_dir = \"conditionC\"\n",
    "# exper_dir = \"rest\"\n",
    "hc_path = f\"../data/eegmap_chunks/{exper_dir}/hc/{exper_dir}_hc.pt\"\n",
    "mcs_path = f\"../data/eegmap_chunks/{exper_dir}/mcs/{exper_dir}_mcs.pt\"\n",
    "uws_path = f\"../data/eegmap_chunks/{exper_dir}/uws/{exper_dir}_uws.pt\"\n",
    "hc = torch.load(hc_path)\n",
    "mcs = torch.load(mcs_path)\n",
    "uws = torch.load(uws_path)\n",
    "print(hc.size())\n",
    "print(mcs.size())\n",
    "print(uws.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15383add",
   "metadata": {},
   "source": [
    "## 1.1 Split Map Data - Cross Fold Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737bd0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231\n",
      "1108\n",
      "1353\n",
      "1218\n",
      "1220\n",
      "1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:50: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b7e3b4175f4c9e9c47ba7424e635a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([975, 2400, 10, 11])\n",
      "torch.Size([243, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([975, 2400, 10, 11])\n",
      "torch.Size([243, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([975, 2400, 10, 11])\n",
      "torch.Size([243, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([975, 2400, 10, 11])\n",
      "torch.Size([243, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([975, 2400, 10, 11])\n",
      "torch.Size([243, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# 定义函数用于保存数据集\n",
    "def save_dataset(data, exper_dir, subset, mode, fold):\n",
    "    torch.save(data, f\"../data/eegmap_split/{exper_dir}/train/{subset}/{exper_dir}_{subset}_{mode}_{fold}.pt\")\n",
    "\n",
    "# test dataset\n",
    "# 生成随机种子\n",
    "torch.manual_seed(32)\n",
    "# 生成随机索引\n",
    "hc_indices = torch.randperm(len(hc))\n",
    "mcs_indices = torch.randperm(len(mcs))\n",
    "uws_indices = torch.randperm(len(uws))\n",
    "test_percentage = 0.1\n",
    "# hc\n",
    "print(len(hc))\n",
    "test_end = int(test_percentage * len(hc))\n",
    "test_hc_indices = hc_indices[:test_end]\n",
    "test_hc = hc[test_hc_indices]\n",
    "torch.save(test_hc, f\"../data/eegmap_split/{exper_dir}/test/hc/{exper_dir}_hc_test.pt\")\n",
    "hc = hc[hc_indices[test_end:]]\n",
    "print(len(hc))\n",
    "# mcs\n",
    "print(len(mcs))\n",
    "test_end = int(test_percentage * len(mcs))\n",
    "test_mcs_indices = mcs_indices[:test_end]\n",
    "test_mcs = mcs[test_mcs_indices]\n",
    "torch.save(test_mcs, f\"../data/eegmap_split/{exper_dir}/test/mcs/{exper_dir}_mcs_test.pt\")\n",
    "mcs = mcs[mcs_indices[test_end:]]\n",
    "print(len(mcs))\n",
    "#uws\n",
    "print(len(uws))\n",
    "test_end = int(test_percentage * len(uws))\n",
    "test_uws_indices = uws_indices[:test_end]\n",
    "test_uws = uws[test_uws_indices]\n",
    "torch.save(test_uws, f\"../data/eegmap_split/{exper_dir}/test/uws/{exper_dir}_uws_test.pt\")\n",
    "uws = uws[uws_indices[test_end:]]\n",
    "print(len(uws))\n",
    "# 清理内存\n",
    "del hc_indices, mcs_indices, uws_indices, test_hc_indices, test_mcs_indices, test_uws_indices\n",
    "del test_hc, test_mcs, test_uws\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# train dataset\n",
    "# 生成随机索引\n",
    "hc_indices = torch.randperm(len(hc))\n",
    "mcs_indices = torch.randperm(len(mcs))\n",
    "uws_indices = torch.randperm(len(uws))\n",
    "fold_num = 5\n",
    "# 划分数据集并保存\n",
    "for fold in tqdm(range(fold_num)):\n",
    "    # 划分 hc 数据集\n",
    "    len_hc = len(hc)\n",
    "    val_size = len_hc // fold_num  # 每一折的验证集大小\n",
    "    val_start = fold * val_size  # 验证集起始索引\n",
    "    val_end = min((fold + 1) * val_size, len_hc)  # 验证集结束索引\n",
    "    train_hc_indices = torch.cat([hc_indices[:val_start], hc_indices[val_end:]])\n",
    "    val_hc_indices = hc_indices[val_start:val_end]\n",
    "    train_hc = hc[train_hc_indices]\n",
    "    val_hc = hc[val_hc_indices]\n",
    "    save_dataset(train_hc, exper_dir, 'hc', 'train', fold)\n",
    "    save_dataset(val_hc, exper_dir, 'hc', 'val', fold)\n",
    "    print(train_hc.size())\n",
    "    print(val_hc.size())\n",
    "    \n",
    "    # 划分 mcs 数据集\n",
    "    len_mcs = len(mcs)\n",
    "    val_size = len_mcs // fold_num\n",
    "    val_start = fold * val_size\n",
    "    val_end = min((fold + 1) * val_size, len_mcs)\n",
    "    train_mcs_indices = torch.cat([mcs_indices[:val_start], mcs_indices[val_end:]])\n",
    "    val_mcs_indices = mcs_indices[val_start:val_end]\n",
    "    train_mcs = mcs[train_mcs_indices]\n",
    "    val_mcs = mcs[val_mcs_indices]\n",
    "    save_dataset(train_mcs, exper_dir, 'mcs', 'train', fold)\n",
    "    save_dataset(val_mcs, exper_dir, 'mcs', 'val', fold)\n",
    "    print(train_mcs.size())\n",
    "    print(val_mcs.size())\n",
    "    \n",
    "    # 划分 uws 数据集\n",
    "    len_uws = len(uws)\n",
    "    val_size = len_uws // fold_num\n",
    "    val_start = fold * val_size\n",
    "    val_end = min((fold + 1) * val_size, len_uws)\n",
    "    train_uws_indices = torch.cat([uws_indices[:val_start], uws_indices[val_end:]])\n",
    "    val_uws_indices = uws_indices[val_start:val_end]\n",
    "    train_uws = uws[train_uws_indices]\n",
    "    val_uws = uws[val_uws_indices]\n",
    "    save_dataset(train_uws, exper_dir, 'uws', 'train', fold)\n",
    "    save_dataset(val_uws, exper_dir, 'uws', 'val', fold)\n",
    "    print(train_uws.size())\n",
    "    print(val_uws.size())\n",
    "    \n",
    "    # 清理内存\n",
    "    del train_hc_indices, train_mcs_indices, train_uws_indices,\n",
    "    del val_hc_indices, val_mcs_indices, val_uws_indices\n",
    "    del train_hc, train_mcs, train_uws, val_hc, val_mcs, val_uws\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# 清理内存\n",
    "del hc_indices, mcs_indices, uws_indices\n",
    "del hc, mcs, uws\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ead6459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([123, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.load(f\"../data/eegmap_split/{exper_dir}/test/hc/{exper_dir}_hc_test.pt\")\n",
    "print(test_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23ded",
   "metadata": {},
   "source": [
    "## 1.2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38980372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94463a3a083490388a28478dc7c9163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionC\\test\\hc\\conditionC_hc_test.pt\n",
      "torch.Size([123, 2400, 10, 11])\n",
      "label_size:torch.Size([123]), label:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6418674aa23840b08cb667520121f54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionC\\test\\mcs\\conditionC_mcs_test.pt\n",
      "torch.Size([135, 2400, 10, 11])\n",
      "label_size:torch.Size([135]), label:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:41: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef616f619257424f897690c88b8cf85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionC\\test\\uws\\conditionC_uws_test.pt\n",
      "torch.Size([122, 2400, 10, 11])\n",
      "label_size:torch.Size([122]), label:1.0\n"
     ]
    }
   ],
   "source": [
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "exper_dir = \"conditionC\"\n",
    "# exper_dir = \"rest\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "# mode: train, test\n",
    "mode = \"train\"\n",
    "# condition_dir: hc, mcs, uws\n",
    "condi_dir = \"hc\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "    label = torch.zeros(data_map.size(0)) # label = 0\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "condi_dir = \"mcs\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "    label = torch.ones(data_map.size(0)) # label = 1, for HC/DOC\n",
    "#     label = torch.zeros(data_map.size(0)) # label = 0, for MCS/UWS\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "condi_dir = \"uws\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "#     label = torch.ones(data_map.size(0)) * 2 # label = 2\n",
    "    label = torch.ones(data_map.size(0)) # label = 1\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cde12",
   "metadata": {},
   "source": [
    "# 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8e967",
   "metadata": {},
   "source": [
    "## Hyperparameters and Related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0a7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "learn_rate = 0.0005\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c43a00",
   "metadata": {},
   "source": [
    "## Ensuring deterministicity through Random seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92d241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "manualSeed = 4\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729b68e",
   "metadata": {},
   "source": [
    "## Setting the optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "682c2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 定义LSTM超参数\n",
    "input_size = 64  # 输入特征维度\n",
    "hidden_size = 64  # 隐藏单元数量\n",
    "num_layers = 2  # LSTM层数\n",
    "output_size = 2  # 输出类别数量\n",
    "model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "# # model.load_state_dict(torch.load(\"../model/Trial2.pt\")) # !!!!!!!!!!!!!!!!!!!!!\n",
    "model = model.to(device)\n",
    "\n",
    "# ==损失函数权重\n",
    "# ======== 二分类HC/DOC\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 887 + 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "total_samples = 887 + 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 887, total_samples / (985 + 879)]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "weights = [total_samples / 887, total_samples / (975 + 879)]\n",
    "\n",
    "# ======== 二分类MCS/UWS\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 985 + 879\n",
    "# condition2\n",
    "# total_samples = 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 985, total_samples / 879]\n",
    "# condition2\n",
    "# weights = [total_samples / 1029, total_samples / 886]\n",
    "# condition3\n",
    "# weights = [total_samples / 975, total_samples / 879]\n",
    "\n",
    "# # ======== 三分类\n",
    "# # 计算总样本数量\n",
    "# # conditionB\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# # 计算每个类别的权重\n",
    "# # conditionB\n",
    "# weights = [total_samples / 929, total_samples / 1029, total_samples / 886]\n",
    "\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae23bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d9378",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66243e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd8543fe4cd4c3e91b44e5fb35c16ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionC\\train\\mcs\\conditionC_mcs_train_4.pt\n",
      "../data/eegmap_split/conditionC\\train\\mcs\\conditionC_mcs_train_4_label.pt\n",
      "../data/eegmap_split/conditionC\\train\\mcs\\conditionC_mcs_val_4.pt\n",
      "../data/eegmap_split/conditionC\\train\\mcs\\conditionC_mcs_val_4_label.pt\n",
      "../data/eegmap_split/conditionC\\train\\uws\\conditionC_uws_train_4.pt\n",
      "../data/eegmap_split/conditionC\\train\\uws\\conditionC_uws_train_4_label.pt\n",
      "../data/eegmap_split/conditionC\\train\\uws\\conditionC_uws_val_4.pt\n",
      "../data/eegmap_split/conditionC\\train\\uws\\conditionC_uws_val_4_label.pt\n",
      "torch.Size([1854, 2400, 10, 11])\n",
      "torch.Size([1854])\n",
      "torch.Size([462, 2400, 10, 11])\n",
      "torch.Size([462])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:220: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a3b72ac019439fb78cb3ed59ea5318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch 0 Training =========\n",
      "tensor([[0.4090, 0.4429]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 138.31700015068054\n",
      "Train steps: 1000, Loss: 0.7102173566818237\n",
      "========= Epoch 0 Testing =========\n",
      "Loss: 71.78729248046875 Accuracy: 0.5099999904632568\n",
      "Loss: 141.45748901367188 Accuracy: 0.5199999809265137\n",
      "Loss: 212.9304656982422 Accuracy: 0.5200000405311584\n",
      "Loss: 282.4778137207031 Accuracy: 0.5149999856948853\n",
      "Total Loss: 328.2361145019531 Total Accuracy: 0.5151515007019043\n",
      "..........Saving the model..........\n",
      "========= Epoch 1 Training =========\n",
      "tensor([[-1.6192, -0.7912]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 320.9880225658417\n",
      "Train steps: 2000, Loss: 0.36251598596572876\n",
      "tensor([[-0.4642, -0.2575]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 458.7530641555786\n",
      "Train steps: 3000, Loss: 0.8018275499343872\n",
      "========= Epoch 1 Testing =========\n",
      "Loss: 68.99330139160156 Accuracy: 0.5899999737739563\n",
      "Loss: 144.15415954589844 Accuracy: 0.5699999928474426\n",
      "Loss: 215.02735900878906 Accuracy: 0.5433333516120911\n",
      "Loss: 283.99609375 Accuracy: 0.5424999594688416\n",
      "Total Loss: 334.9259948730469 Total Accuracy: 0.5151515007019043\n",
      "..........Saving the model..........\n",
      "========= Epoch 2 Training =========\n",
      "tensor([[-0.2308, -0.4085]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 640.0280220508575\n",
      "Train steps: 4000, Loss: 0.7859331369400024\n",
      "tensor([[-1.5659, -0.9993]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 777.522047996521\n",
      "Train steps: 5000, Loss: 0.44944238662719727\n",
      "========= Epoch 2 Testing =========\n",
      "Loss: 75.84640502929688 Accuracy: 0.4699999988079071\n",
      "Loss: 144.56558227539062 Accuracy: 0.5349999666213989\n",
      "Loss: 210.1018829345703 Accuracy: 0.5800000429153442\n",
      "Loss: 279.3641662597656 Accuracy: 0.5774999856948853\n",
      "Total Loss: 323.46868896484375 Total Accuracy: 0.5757575631141663\n",
      "..........Saving the model..........\n",
      "========= Epoch 3 Training =========\n",
      "tensor([[2.8543, 1.0126]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 959.4590485095978\n",
      "Train steps: 6000, Loss: 0.14716790616512299\n",
      "tensor([[-2.8332,  0.0743]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1096.54904961586\n",
      "Train steps: 7000, Loss: 0.053174324333667755\n",
      "========= Epoch 3 Testing =========\n",
      "Loss: 58.2191047668457 Accuracy: 0.6800000071525574\n",
      "Loss: 107.48070526123047 Accuracy: 0.7450000047683716\n",
      "Loss: 161.23609924316406 Accuracy: 0.7333333492279053\n",
      "Loss: 219.86581420898438 Accuracy: 0.7224999666213989\n",
      "Total Loss: 252.84912109375 Total Accuracy: 0.7294372320175171\n",
      "..........Saving the model..........\n",
      "========= Epoch 4 Training =========\n",
      "tensor([[-6.0787, -0.8143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1267.9275591373444\n",
      "Train steps: 8000, Loss: 0.005159278400242329\n",
      "tensor([[7.0317, 0.5074]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1396.5745587348938\n",
      "Train steps: 9000, Loss: 0.0014663906767964363\n",
      "========= Epoch 4 Testing =========\n",
      "Loss: 28.033618927001953 Accuracy: 0.8399999737739563\n",
      "Loss: 56.87021255493164 Accuracy: 0.8349999785423279\n",
      "Loss: 87.27985382080078 Accuracy: 0.8333333730697632\n",
      "Loss: 111.24256134033203 Accuracy: 0.8399999737739563\n",
      "Total Loss: 133.43173217773438 Total Accuracy: 0.8354978561401367\n",
      "..........Saving the model..........\n",
      "========= Epoch 5 Training =========\n",
      "tensor([[-12.5814,   0.4285]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1562.4556143283844\n",
      "Train steps: 10000, Loss: 2.264974000354414e-06\n",
      "tensor([[-3.5033,  6.9553]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1693.638644695282\n",
      "Train steps: 11000, Loss: 2.8729025871143676e-05\n",
      "========= Epoch 5 Testing =========\n",
      "Loss: 24.51764488220215 Accuracy: 0.8899999856948853\n",
      "Loss: 51.83415603637695 Accuracy: 0.8899999856948853\n",
      "Loss: 87.75982666015625 Accuracy: 0.9000000357627869\n",
      "Loss: 103.24307250976562 Accuracy: 0.9149999618530273\n",
      "Total Loss: 119.658935546875 Total Accuracy: 0.911255419254303\n",
      "..........Saving the model..........\n",
      "========= Epoch 6 Training =========\n",
      "tensor([[ 5.2690, -5.4831]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1860.0856556892395\n",
      "Train steps: 12000, Loss: 2.1457441107486375e-05\n",
      "========= Epoch 6 Testing =========\n",
      "Loss: 10.435829162597656 Accuracy: 0.9399999976158142\n",
      "Loss: 18.006567001342773 Accuracy: 0.949999988079071\n",
      "Loss: 44.63640594482422 Accuracy: 0.9366666674613953\n",
      "Loss: 62.0339241027832 Accuracy: 0.9325000047683716\n",
      "Total Loss: 78.92621612548828 Total Accuracy: 0.9285714030265808\n",
      "..........Saving the model..........\n",
      "========= Epoch 7 Training =========\n",
      "tensor([[-13.0188,   6.1544]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2025.0631971359253\n",
      "Train steps: 13000, Loss: 0.0\n",
      "tensor([[-2.5618, -0.6330]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2156.06227517128\n",
      "Train steps: 14000, Loss: 0.13568943738937378\n",
      "========= Epoch 7 Testing =========\n",
      "Loss: 43.52295684814453 Accuracy: 0.8499999642372131\n",
      "Loss: 84.84814453125 Accuracy: 0.875\n",
      "Loss: 114.9823226928711 Accuracy: 0.8833333253860474\n",
      "Loss: 136.38368225097656 Accuracy: 0.8899999856948853\n",
      "Total Loss: 155.31784057617188 Total Accuracy: 0.8896104097366333\n",
      "..........Saving the model..........\n",
      "========= Epoch 8 Training =========\n",
      "tensor([[-17.0972,   2.3786]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2320.944288253784\n",
      "Train steps: 15000, Loss: 0.0\n",
      "tensor([[ 1.1894, -7.5719]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2453.1993005275726\n",
      "Train steps: 16000, Loss: 0.00015662873920518905\n",
      "========= Epoch 8 Testing =========\n",
      "Loss: 15.493265151977539 Accuracy: 0.9599999785423279\n",
      "Loss: 28.696863174438477 Accuracy: 0.9699999690055847\n",
      "Loss: 36.90186309814453 Accuracy: 0.9633333683013916\n",
      "Loss: 47.97018814086914 Accuracy: 0.9574999809265137\n",
      "Total Loss: 53.01490783691406 Total Accuracy: 0.9588744640350342\n",
      "..........Saving the model..........\n",
      "========= Epoch 9 Training =========\n",
      "tensor([[ 9.0383, 24.4689]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2620.442354440689\n",
      "Train steps: 17000, Loss: 2.3841856489070778e-07\n",
      "tensor([[ -7.7115, -16.6221]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2748.8163776397705\n",
      "Train steps: 18000, Loss: 0.00013493580627255142\n",
      "========= Epoch 9 Testing =========\n",
      "Loss: 37.07552719116211 Accuracy: 0.85999995470047\n",
      "Loss: 69.39031219482422 Accuracy: 0.85999995470047\n",
      "Loss: 92.75529479980469 Accuracy: 0.8733333349227905\n",
      "Loss: 124.01728820800781 Accuracy: 0.8824999928474426\n",
      "Total Loss: 144.3197479248047 Total Accuracy: 0.8809523582458496\n",
      "..........Saving the model..........\n",
      "========= Epoch 10 Training =========\n",
      "tensor([[ -3.5710, -13.2836]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2912.9113759994507\n",
      "Train steps: 19000, Loss: 6.05564855504781e-05\n",
      "tensor([[ -6.0836, -22.9685]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3044.078391313553\n",
      "Train steps: 20000, Loss: 0.0\n",
      "========= Epoch 10 Testing =========\n",
      "Loss: 17.635848999023438 Accuracy: 0.9199999570846558\n",
      "Loss: 28.071025848388672 Accuracy: 0.9350000023841858\n",
      "Loss: 33.91852951049805 Accuracy: 0.9466667175292969\n",
      "Loss: 41.89537048339844 Accuracy: 0.9524999856948853\n",
      "Total Loss: 47.48992919921875 Total Accuracy: 0.948051929473877\n",
      "..........Saving the model..........\n",
      "========= Epoch 11 Training =========\n",
      "tensor([[ -8.7962, -24.8057]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3214.0054426193237\n",
      "Train steps: 21000, Loss: 1.1920928244535389e-07\n",
      "tensor([[ -7.5676, -24.2336]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3343.5024688243866\n",
      "Train steps: 22000, Loss: 0.0\n",
      "========= Epoch 11 Testing =========\n",
      "Loss: 12.939651489257812 Accuracy: 0.8999999761581421\n",
      "Loss: 24.038795471191406 Accuracy: 0.9249999523162842\n",
      "Loss: 47.33634948730469 Accuracy: 0.9166666865348816\n",
      "Loss: 52.59906768798828 Accuracy: 0.9300000071525574\n",
      "Total Loss: 55.37803649902344 Total Accuracy: 0.9372294545173645\n",
      "..........Saving the model..........\n",
      "========= Epoch 12 Training =========\n",
      "tensor([[ -7.2569, -26.6993]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3508.7214679718018\n",
      "Train steps: 23000, Loss: 0.0\n",
      "tensor([[ -0.2607, -14.3745]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3638.6914830207825\n",
      "Train steps: 24000, Loss: 7.152555099310121e-07\n",
      "========= Epoch 12 Testing =========\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 29.11870765686035 Accuracy: 0.8899999856948853\n",
      "Loss: 53.650081634521484 Accuracy: 0.8949999809265137\n",
      "Loss: 92.91840362548828 Accuracy: 0.8866667151451111\n",
      "Loss: 123.23455810546875 Accuracy: 0.8899999856948853\n",
      "Total Loss: 134.20249938964844 Total Accuracy: 0.8917748928070068\n",
      "..........Saving the model..........\n",
      "========= Epoch 13 Training =========\n",
      "tensor([[ 5.7406, 17.6428]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3803.937484025955\n",
      "Train steps: 25000, Loss: 6.794905857532285e-06\n",
      "========= Epoch 13 Testing =========\n",
      "Loss: 19.691431045532227 Accuracy: 0.9399999976158142\n",
      "Loss: 40.01441955566406 Accuracy: 0.9350000023841858\n",
      "Loss: 51.35788345336914 Accuracy: 0.9433333277702332\n",
      "Loss: 78.27337646484375 Accuracy: 0.9325000047683716\n",
      "Total Loss: 92.85108947753906 Total Accuracy: 0.9307359457015991\n",
      "..........Saving the model..........\n",
      "========= Epoch 14 Training =========\n",
      "tensor([[-0.5986, 19.4012]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3968.608506679535\n",
      "Train steps: 26000, Loss: 0.0\n",
      "tensor([[ 3.9841, 14.9393]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4097.510507583618\n",
      "Train steps: 27000, Loss: 1.7523612768854946e-05\n",
      "========= Epoch 14 Testing =========\n",
      "Loss: 16.06108856201172 Accuracy: 0.9399999976158142\n",
      "Loss: 37.20029830932617 Accuracy: 0.9350000023841858\n",
      "Loss: 47.40925598144531 Accuracy: 0.9433333277702332\n",
      "Loss: 68.85192108154297 Accuracy: 0.9399999976158142\n",
      "Total Loss: 92.13072967529297 Total Accuracy: 0.9350649118423462\n",
      "..........Saving the model..........\n",
      "========= Epoch 15 Training =========\n",
      "tensor([[-4.5433, 12.4024]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4260.400508642197\n",
      "Train steps: 28000, Loss: 0.0\n",
      "tensor([[ -4.1610, -23.8045]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4390.750506639481\n",
      "Train steps: 29000, Loss: 0.0\n",
      "========= Epoch 15 Testing =========\n",
      "Loss: 20.787349700927734 Accuracy: 0.8999999761581421\n",
      "Loss: 37.17839813232422 Accuracy: 0.9199999570846558\n",
      "Loss: 63.95053482055664 Accuracy: 0.9233333468437195\n",
      "Loss: 74.17405700683594 Accuracy: 0.9325000047683716\n",
      "Total Loss: 82.68815612792969 Total Accuracy: 0.9350649118423462\n",
      "..........Saving the model..........\n",
      "========= Epoch 16 Training =========\n",
      "tensor([[ -4.7808, -21.5347]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4557.467532873154\n",
      "Train steps: 30000, Loss: 0.0\n",
      "tensor([[ 4.7510, 29.9641]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4686.021533727646\n",
      "Train steps: 31000, Loss: 0.0\n",
      "========= Epoch 16 Testing =========\n",
      "Loss: 12.524869918823242 Accuracy: 0.9599999785423279\n",
      "Loss: 21.761959075927734 Accuracy: 0.9599999785423279\n",
      "Loss: 34.98764419555664 Accuracy: 0.9566667079925537\n",
      "Loss: 48.7881965637207 Accuracy: 0.9474999904632568\n",
      "Total Loss: 57.610801696777344 Total Accuracy: 0.948051929473877\n",
      "..........Saving the model..........\n",
      "========= Epoch 17 Training =========\n",
      "tensor([[  0.2643, -22.6143]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4850.317533016205\n",
      "Train steps: 32000, Loss: 0.0\n",
      "tensor([[ 2.2977, 30.3909]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 4980.470717668533\n",
      "Train steps: 33000, Loss: 0.0\n",
      "========= Epoch 17 Testing =========\n",
      "Loss: 19.9146785736084 Accuracy: 0.9300000071525574\n",
      "Loss: 41.51841354370117 Accuracy: 0.9350000023841858\n",
      "Loss: 61.19092559814453 Accuracy: 0.9366666674613953\n",
      "Loss: 78.44558715820312 Accuracy: 0.9325000047683716\n",
      "Total Loss: 85.49287414550781 Total Accuracy: 0.9350649118423462\n",
      "..........Saving the model..........\n",
      "========= Epoch 18 Training =========\n",
      "tensor([[ -4.6890, -26.1806]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 5144.499245405197\n",
      "Train steps: 34000, Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "# exper_dir = \"rest\"\n",
    "exper_dir = \"conditionC\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "classification = \"mcs_uws\"\n",
    "fold_num = 1\n",
    "for fold in tqdm(range(fold_num)):\n",
    "    # train num folds\n",
    "    fold = 4\n",
    "    # -- prepare datasets\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "#     # ---- hc\n",
    "#     dataset = MyData(root_dir, \"train\", \"hc\") # hc\n",
    "#     # find the fold file\n",
    "#     count = 0\n",
    "#     for person in range(len(dataset)):\n",
    "#         filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "#         # extract the pure name of the file\n",
    "#         parts = filename.split(\"\\\\\")\n",
    "#         file_name = parts[-1]\n",
    "#         name_without_extension = file_name.split(\".\")[0]\n",
    "#         # label or data\n",
    "#         file_last = name_without_extension.split(\"_\")[-1]\n",
    "#         if file_last.isdigit(): # data\n",
    "#             # is this fold or not\n",
    "#             if int(file_last) == fold: # yes\n",
    "#                 print(filename)\n",
    "#                 count = count + 1\n",
    "#                 data_map = torch.load(filename)\n",
    "#                 # train or valid\n",
    "#                 if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         train_data.append(data_map[i])\n",
    "#                 elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         test_data.append(data_map[i])\n",
    "#                 if count == 4:\n",
    "#                     del data_map\n",
    "#                     gc.collect()\n",
    "#                     torch.cuda.empty_cache() \n",
    "#                     break\n",
    "#             else:   # not\n",
    "#                 pass\n",
    "#         else: # label\n",
    "#             # is this fold or not\n",
    "#             file_last = name_without_extension.split(\"_\")[-2]\n",
    "#             if int(file_last) == fold: # yes\n",
    "#                 print(filename)\n",
    "#                 count = count + 1\n",
    "#                 data_map = torch.load(filename)\n",
    "#                 # train or valid\n",
    "#                 if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         train_label.append(data_map[i])\n",
    "#                 elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         test_label.append(data_map[i])\n",
    "#                 if count == 4:\n",
    "#                     del data_map\n",
    "#                     gc.collect()\n",
    "#                     torch.cuda.empty_cache() \n",
    "#                     break\n",
    "#             else:   # not\n",
    "#                 pass\n",
    "#         del filename, parts, file_name, name_without_extension, file_last\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()    \n",
    "    # ---- mcs\n",
    "    dataset = MyData(root_dir, \"train\", \"mcs\") # mcs\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            # is this fold or not\n",
    "            file_last = name_without_extension.split(\"_\")[-2]\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_label.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_label.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "    # ---- uws\n",
    "    dataset = MyData(root_dir, \"train\", \"uws\") # uws\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            # is this fold or not\n",
    "            file_last = name_without_extension.split(\"_\")[-2]\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_label.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_label.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass \n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "    print(torch.stack(train_data).size())\n",
    "    print(torch.stack(train_label).size())\n",
    "    print(torch.stack(test_data).size())\n",
    "    print(torch.stack(test_label).size())\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "    \n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.stack(train_label)\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    # train dataset\n",
    "    train_td = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(train_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    # test dataset\n",
    "    test_td = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(test_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    del train_data, train_label, test_data, test_label, train_td, test_td\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # set mode for each fold\n",
    "    model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40, 60], gamma=0.2)\n",
    "    # -- start training\n",
    "    start_time = time.time()\n",
    "    # train and test step records\n",
    "    total_train_step = 0\n",
    "    total_test_step = 0\n",
    "    min_test_loss = 1000\n",
    "    # add Tensorboard\n",
    "    writer_train = SummaryWriter(f\"../logs/{classification}/{exper_dir}_CNN_spa_lstm/logs_train_{fold}\")\n",
    "    writer_valid = SummaryWriter(f\"../logs/{classification}/{exper_dir}_CNN_spa_lstm/logs_valid_{fold}\")\n",
    "    writer_valid_acc = SummaryWriter(f\"../logs/{classification}/{exper_dir}_CNN_spa_lstm/logs_valid_acc_{fold}\")\n",
    "    for i in tqdm(range(num_epochs)):  \n",
    "        print(f\"========= Epoch {i} Training =========\")\n",
    "        # train steps\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            # x, y\n",
    "            data_map, label=data\n",
    "            data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "            label_int = label.long()\n",
    "            data_map_reshaped=data_map_reshaped.to(device)\n",
    "            label_int=label_int.to(device)\n",
    "            del data_map, label\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # y_pred\n",
    "            label_pred = model(data_map_reshaped)\n",
    "            # Loss Computation and Optimization\n",
    "            loss = criterion(label_pred,label_int)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # draw tensorboard\n",
    "            total_train_step = total_train_step + 1\n",
    "            # print info\n",
    "            if total_train_step % 1000 == 0:\n",
    "                end_time = time.time()\n",
    "                print(label_pred)\n",
    "                print(f\"Train time: {end_time - start_time}\")\n",
    "                print(f\"Train steps: {total_train_step}, Loss: {loss.item()}\")\n",
    "            writer_train.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "            # Clear gpu\n",
    "            del data, data_map_reshaped, label_int, label_pred, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluation and save the best model\n",
    "        print(f\"========= Epoch {i} Testing =========\")\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        test_count = 0\n",
    "        total_test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                test_count = test_count + 1\n",
    "                # x, y\n",
    "                data_map, label=data\n",
    "                data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "                label_int = label.long()\n",
    "                data_map_reshaped = data_map_reshaped.to(device)\n",
    "                label_int = label_int.to(device)\n",
    "                del data_map, label\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # y_pred\n",
    "                label_pred_test = model(data_map_reshaped)\n",
    "                loss = criterion(label_pred_test,label_int)\n",
    "#                 print(label_pred_test)\n",
    "                # accuracy \n",
    "                total_test_acc = total_test_acc + ((label_pred_test.argmax(1)) == label_int).sum()\n",
    "                # draw tensorboad\n",
    "                total_test_loss = total_test_loss + loss\n",
    "                if test_count % 100 == 0:\n",
    "                    print(f\"Loss: {total_test_loss} Accuracy: {total_test_acc/test_count}\")\n",
    "                # Clear gpu\n",
    "                del data_map_reshaped, label_int, label_pred_test, loss, data\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"Total Loss: {total_test_loss} Total Accuracy: {total_test_acc/test_count}\")\n",
    "        writer_valid.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "        writer_valid_acc.add_scalar(\"test_acc\", total_test_acc/test_count, total_test_step)\n",
    "        total_test_step = total_test_step + 1\n",
    "        print(\"..........Saving the model..........\")\n",
    "        torch.save(model.state_dict(),f\"../model/{classification}/{exper_dir}_CNN_spa_lstm/Fold{fold}_Epoch{i}.pt\") \n",
    "#         if total_test_loss < min_test_loss:\n",
    "#             min_test_loss = total_test_loss\n",
    "#             print(\"..........Saving the model..........\")\n",
    "#             torch.save(model.state_dict(),f\"../model/{exper_dir}/Fold{fold}_Epoch{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fa7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, train_label, test_data, test_label\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d621c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([2])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([1])\n",
    "print(test.size())\n",
    "test = torch.tensor([test.item(),test.item()])\n",
    "print(test.size())\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "322cc097",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_map_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-25223a449931>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mdata_map_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mlabel_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_map_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Clear gpu\n",
    "del optimizer\n",
    "del scheduler\n",
    "del criterion\n",
    "del model\n",
    "del data_map_pred\n",
    "del label\n",
    "del label_pred\n",
    "del label_pred_test\n",
    "del loss\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "528060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
