{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089503e4",
   "metadata": {},
   "source": [
    "# Head Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a0232fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import MyData  # self-made\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm # View procedure\n",
    "import os\n",
    "import scipy.io\n",
    "from random import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from network_cnn_lstm import MyNetwork\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d854be2",
   "metadata": {},
   "source": [
    "# 1. Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521251cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1231, 2400, 10, 11])\n",
      "torch.Size([1367, 2400, 10, 11])\n",
      "torch.Size([1219, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "exper_dir = \"conditionA\"\n",
    "# exper_dir = \"rest\"\n",
    "hc_path = f\"../data/eegmap_chunks/{exper_dir}/hc/{exper_dir}_hc.pt\"\n",
    "mcs_path = f\"../data/eegmap_chunks/{exper_dir}/mcs/{exper_dir}_mcs.pt\"\n",
    "uws_path = f\"../data/eegmap_chunks/{exper_dir}/uws/{exper_dir}_uws.pt\"\n",
    "hc = torch.load(hc_path)\n",
    "mcs = torch.load(mcs_path)\n",
    "uws = torch.load(uws_path)\n",
    "print(hc.size())\n",
    "print(mcs.size())\n",
    "print(uws.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15383add",
   "metadata": {},
   "source": [
    "## 1.1 Split Map Data - Cross Fold Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "737bd0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1231\n",
      "1108\n",
      "1367\n",
      "1231\n",
      "1219\n",
      "1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:50: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b72b1829b814845b08338bace0c988d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([985, 2400, 10, 11])\n",
      "torch.Size([246, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([985, 2400, 10, 11])\n",
      "torch.Size([246, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([985, 2400, 10, 11])\n",
      "torch.Size([246, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([985, 2400, 10, 11])\n",
      "torch.Size([246, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n",
      "torch.Size([887, 2400, 10, 11])\n",
      "torch.Size([221, 2400, 10, 11])\n",
      "torch.Size([985, 2400, 10, 11])\n",
      "torch.Size([246, 2400, 10, 11])\n",
      "torch.Size([879, 2400, 10, 11])\n",
      "torch.Size([219, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "# 定义函数用于保存数据集\n",
    "def save_dataset(data, exper_dir, subset, mode, fold):\n",
    "    torch.save(data, f\"../data/eegmap_split/{exper_dir}/train/{subset}/{exper_dir}_{subset}_{mode}_{fold}.pt\")\n",
    "\n",
    "# test dataset\n",
    "# 生成随机种子\n",
    "torch.manual_seed(32)\n",
    "# 生成随机索引\n",
    "hc_indices = torch.randperm(len(hc))\n",
    "mcs_indices = torch.randperm(len(mcs))\n",
    "uws_indices = torch.randperm(len(uws))\n",
    "test_percentage = 0.1\n",
    "# hc\n",
    "print(len(hc))\n",
    "test_end = int(test_percentage * len(hc))\n",
    "test_hc_indices = hc_indices[:test_end]\n",
    "test_hc = hc[test_hc_indices]\n",
    "torch.save(test_hc, f\"../data/eegmap_split/{exper_dir}/test/hc/{exper_dir}_hc_test.pt\")\n",
    "hc = hc[hc_indices[test_end:]]\n",
    "print(len(hc))\n",
    "# mcs\n",
    "print(len(mcs))\n",
    "test_end = int(test_percentage * len(mcs))\n",
    "test_mcs_indices = mcs_indices[:test_end]\n",
    "test_mcs = mcs[test_mcs_indices]\n",
    "torch.save(test_mcs, f\"../data/eegmap_split/{exper_dir}/test/mcs/{exper_dir}_mcs_test.pt\")\n",
    "mcs = mcs[mcs_indices[test_end:]]\n",
    "print(len(mcs))\n",
    "#uws\n",
    "print(len(uws))\n",
    "test_end = int(test_percentage * len(uws))\n",
    "test_uws_indices = uws_indices[:test_end]\n",
    "test_uws = uws[test_uws_indices]\n",
    "torch.save(test_uws, f\"../data/eegmap_split/{exper_dir}/test/uws/{exper_dir}_uws_test.pt\")\n",
    "uws = uws[uws_indices[test_end:]]\n",
    "print(len(uws))\n",
    "# 清理内存\n",
    "del hc_indices, mcs_indices, uws_indices, test_hc_indices, test_mcs_indices, test_uws_indices\n",
    "del test_hc, test_mcs, test_uws\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# train dataset\n",
    "# 生成随机索引\n",
    "hc_indices = torch.randperm(len(hc))\n",
    "mcs_indices = torch.randperm(len(mcs))\n",
    "uws_indices = torch.randperm(len(uws))\n",
    "fold_num = 5\n",
    "# 划分数据集并保存\n",
    "for fold in tqdm(range(fold_num)):\n",
    "    # 划分 hc 数据集\n",
    "    len_hc = len(hc)\n",
    "    val_size = len_hc // fold_num  # 每一折的验证集大小\n",
    "    val_start = fold * val_size  # 验证集起始索引\n",
    "    val_end = min((fold + 1) * val_size, len_hc)  # 验证集结束索引\n",
    "    train_hc_indices = torch.cat([hc_indices[:val_start], hc_indices[val_end:]])\n",
    "    val_hc_indices = hc_indices[val_start:val_end]\n",
    "    train_hc = hc[train_hc_indices]\n",
    "    val_hc = hc[val_hc_indices]\n",
    "    save_dataset(train_hc, exper_dir, 'hc', 'train', fold)\n",
    "    save_dataset(val_hc, exper_dir, 'hc', 'val', fold)\n",
    "    print(train_hc.size())\n",
    "    print(val_hc.size())\n",
    "    \n",
    "    # 划分 mcs 数据集\n",
    "    len_mcs = len(mcs)\n",
    "    val_size = len_mcs // fold_num\n",
    "    val_start = fold * val_size\n",
    "    val_end = min((fold + 1) * val_size, len_mcs)\n",
    "    train_mcs_indices = torch.cat([mcs_indices[:val_start], mcs_indices[val_end:]])\n",
    "    val_mcs_indices = mcs_indices[val_start:val_end]\n",
    "    train_mcs = mcs[train_mcs_indices]\n",
    "    val_mcs = mcs[val_mcs_indices]\n",
    "    save_dataset(train_mcs, exper_dir, 'mcs', 'train', fold)\n",
    "    save_dataset(val_mcs, exper_dir, 'mcs', 'val', fold)\n",
    "    print(train_mcs.size())\n",
    "    print(val_mcs.size())\n",
    "    \n",
    "    # 划分 uws 数据集\n",
    "    len_uws = len(uws)\n",
    "    val_size = len_uws // fold_num\n",
    "    val_start = fold * val_size\n",
    "    val_end = min((fold + 1) * val_size, len_uws)\n",
    "    train_uws_indices = torch.cat([uws_indices[:val_start], uws_indices[val_end:]])\n",
    "    val_uws_indices = uws_indices[val_start:val_end]\n",
    "    train_uws = uws[train_uws_indices]\n",
    "    val_uws = uws[val_uws_indices]\n",
    "    save_dataset(train_uws, exper_dir, 'uws', 'train', fold)\n",
    "    save_dataset(val_uws, exper_dir, 'uws', 'val', fold)\n",
    "    print(train_uws.size())\n",
    "    print(val_uws.size())\n",
    "    \n",
    "    # 清理内存\n",
    "    del train_hc_indices, train_mcs_indices, train_uws_indices,\n",
    "    del val_hc_indices, val_mcs_indices, val_uws_indices\n",
    "    del train_hc, train_mcs, train_uws, val_hc, val_mcs, val_uws\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# 清理内存\n",
    "del hc_indices, mcs_indices, uws_indices\n",
    "del hc, mcs, uws\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ead6459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([129, 2400, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.load(f\"../data/eegmap_split/{exper_dir}/test/hc/{exper_dir}_hc_test.pt\")\n",
    "print(test_data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c23ded",
   "metadata": {},
   "source": [
    "## 1.2 Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38980372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157761b18c24f548172cee93cc8400a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionA\\test\\hc\\conditionA_hc_test.pt\n",
      "torch.Size([123, 2400, 10, 11])\n",
      "label_size:torch.Size([123]), label:0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6819d89cf44449bc4084a8fbb3563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionA\\test\\mcs\\conditionA_mcs_test.pt\n",
      "torch.Size([136, 2400, 10, 11])\n",
      "label_size:torch.Size([136]), label:1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6948bf2c7346b7b98061430881e094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionA\\test\\uws\\conditionA_uws_test.pt\n",
      "torch.Size([121, 2400, 10, 11])\n",
      "label_size:torch.Size([121]), label:2.0\n"
     ]
    }
   ],
   "source": [
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "exper_dir = \"conditionA\"\n",
    "# exper_dir = \"rest\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "# mode: train, test\n",
    "mode = \"test\"\n",
    "# condition_dir: hc, mcs, uws\n",
    "condi_dir = \"hc\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "    label = torch.zeros(data_map.size(0)) # label = 0\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "condi_dir = \"mcs\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "    label = torch.ones(data_map.size(0)) # label = 1\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "condi_dir = \"uws\"\n",
    "dataset = MyData(root_dir, mode, condi_dir) # eg.conditionA/hc\n",
    "for person in tqdm(range(len(dataset))):\n",
    "    filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "    data_map = torch.load(filename)\n",
    "    print(filename)\n",
    "    print(data_map.size())\n",
    "    name_without_extension = os.path.splitext(os.path.basename(filename))[0]\n",
    "    label = torch.ones(data_map.size(0)) * 2 # label = 2\n",
    "#     label = torch.ones(data_map.size(0))\n",
    "    save_path = f\"../data/eegmap_split/{exper_dir}/{mode}/{condi_dir}/{name_without_extension}_label.pt\"\n",
    "    torch.save(label, save_path)\n",
    "    print(f\"label_size:{label.size()}, label:{label[0]}\")\n",
    "    del label,data_map,filename\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4cde12",
   "metadata": {},
   "source": [
    "# 2. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8e967",
   "metadata": {},
   "source": [
    "## Hyperparameters and Related parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca0a7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "learn_rate = 0.001\n",
    "num_epochs = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c43a00",
   "metadata": {},
   "source": [
    "## Ensuring deterministicity through Random seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e92d241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "manualSeed = 4\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729b68e",
   "metadata": {},
   "source": [
    "## Setting the optimizer and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682c2f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 定义LSTM超参数\n",
    "input_size = 64  # 输入特征维度\n",
    "hidden_size = 64  # 隐藏单元数量\n",
    "num_layers = 2  # LSTM层数\n",
    "output_size = 2  # 输出类别数量\n",
    "model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "# # model.load_state_dict(torch.load(\"../model/Trial2.pt\")) # !!!!!!!!!!!!!!!!!!!!!\n",
    "model = model.to(device)\n",
    "\n",
    "# ==损失函数权重\n",
    "# ======== 二分类HC/DOC\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 887 + 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 887 + 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 887, total_samples / (985 + 879)]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "# weights = [total_samples / 887, total_samples / (975 + 879)]\n",
    "\n",
    "# ======== 二分类MCS/UWS\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "total_samples = 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 887 + 975 + 879\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "weights = [total_samples / 985, total_samples / 879]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "# weights = [total_samples / 887, total_samples / (975 + 879)]\n",
    "\n",
    "# # ======== 三分类\n",
    "# # 计算总样本数量\n",
    "# # conditionB\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# # 计算每个类别的权重\n",
    "# # conditionB\n",
    "# weights = [total_samples / 929, total_samples / 1029, total_samples / 886]\n",
    "\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae23bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d9378",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66243e2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf8f00af84a416391d3c084f8384289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/conditionA\\train\\mcs\\conditionA_mcs_train_0.pt\n",
      "../data/eegmap_split/conditionA\\train\\mcs\\conditionA_mcs_train_0_label.pt\n",
      "../data/eegmap_split/conditionA\\train\\mcs\\conditionA_mcs_val_0.pt\n",
      "../data/eegmap_split/conditionA\\train\\mcs\\conditionA_mcs_val_0_label.pt\n",
      "../data/eegmap_split/conditionA\\train\\uws\\conditionA_uws_train_0.pt\n",
      "../data/eegmap_split/conditionA\\train\\uws\\conditionA_uws_train_0_label.pt\n",
      "../data/eegmap_split/conditionA\\train\\uws\\conditionA_uws_val_0.pt\n",
      "../data/eegmap_split/conditionA\\train\\uws\\conditionA_uws_val_0_label.pt\n",
      "torch.Size([1864, 2400, 10, 11])\n",
      "torch.Size([1864])\n",
      "torch.Size([465, 2400, 10, 11])\n",
      "torch.Size([465])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:220: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5ecbb15a30404daeabce5a15bcd4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch 0 Training =========\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8fe3354fc3bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;31m# draw tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED"
     ]
    }
   ],
   "source": [
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "# exper_dir = \"rest\"\n",
    "exper_dir = \"conditionA\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "classification = \"mcs_uws\"\n",
    "fold_num = 1\n",
    "for fold in tqdm(range(fold_num)):\n",
    "    # train num folds\n",
    "    fold = 0\n",
    "    # -- prepare datasets\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "#     # ---- hc\n",
    "#     dataset = MyData(root_dir, \"train\", \"hc\") # hc\n",
    "#     # find the fold file\n",
    "#     count = 0\n",
    "#     for person in range(len(dataset)):\n",
    "#         filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "#         # extract the pure name of the file\n",
    "#         parts = filename.split(\"\\\\\")\n",
    "#         file_name = parts[-1]\n",
    "#         name_without_extension = file_name.split(\".\")[0]\n",
    "#         # label or data\n",
    "#         file_last = name_without_extension.split(\"_\")[-1]\n",
    "#         if file_last.isdigit(): # data\n",
    "#             # is this fold or not\n",
    "#             if int(file_last) == fold: # yes\n",
    "#                 print(filename)\n",
    "#                 count = count + 1\n",
    "#                 data_map = torch.load(filename)\n",
    "#                 # train or valid\n",
    "#                 if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         train_data.append(data_map[i])\n",
    "#                 elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         test_data.append(data_map[i])\n",
    "#                 if count == 4:\n",
    "#                     del data_map\n",
    "#                     gc.collect()\n",
    "#                     torch.cuda.empty_cache() \n",
    "#                     break\n",
    "#             else:   # not\n",
    "#                 pass\n",
    "#         else: # label\n",
    "#             # is this fold or not\n",
    "#             file_last = name_without_extension.split(\"_\")[-2]\n",
    "#             if int(file_last) == fold: # yes\n",
    "#                 print(filename)\n",
    "#                 count = count + 1\n",
    "#                 data_map = torch.load(filename)\n",
    "#                 # train or valid\n",
    "#                 if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         train_label.append(data_map[i])\n",
    "#                 elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "#                     for i in range(data_map.size(0)):\n",
    "#                         test_label.append(data_map[i])\n",
    "#                 if count == 4:\n",
    "#                     del data_map\n",
    "#                     gc.collect()\n",
    "#                     torch.cuda.empty_cache() \n",
    "#                     break\n",
    "#             else:   # not\n",
    "#                 pass\n",
    "#         del filename, parts, file_name, name_without_extension, file_last\n",
    "#         gc.collect()\n",
    "#         torch.cuda.empty_cache()    \n",
    "    # ---- mcs\n",
    "    dataset = MyData(root_dir, \"train\", \"mcs\") # mcs\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            # is this fold or not\n",
    "            file_last = name_without_extension.split(\"_\")[-2]\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_label.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_label.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "    # ---- uws\n",
    "    dataset = MyData(root_dir, \"train\", \"uws\") # uws\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            # is this fold or not\n",
    "            file_last = name_without_extension.split(\"_\")[-2]\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or valid\n",
    "                if name_without_extension.split(\"_\")[-3] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_label.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-3] == \"val\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_label.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass \n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "    print(torch.stack(train_data).size())\n",
    "    print(torch.stack(train_label).size())\n",
    "    print(torch.stack(test_data).size())\n",
    "    print(torch.stack(test_label).size())\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "    \n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.stack(train_label)\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    # train dataset\n",
    "    train_td = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(train_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    # test dataset\n",
    "    test_td = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(test_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    del train_data, train_label, test_data, test_label, train_td, test_td\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # set mode for each fold\n",
    "    model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40, 60], gamma=0.2)\n",
    "    # -- start training\n",
    "    start_time = time.time()\n",
    "    # train and test step records\n",
    "    total_train_step = 0\n",
    "    total_test_step = 0\n",
    "    min_test_loss = 1000\n",
    "    # add Tensorboard\n",
    "    writer_train = SummaryWriter(f\"../logs/{classification}/{exper_dir}/logs_train_{fold}\")\n",
    "    writer_valid = SummaryWriter(f\"../logs/{classification}/{exper_dir}/logs_valid_{fold}\")\n",
    "    writer_valid_acc = SummaryWriter(f\"../logs/{classification}/{exper_dir}/logs_valid_acc_{fold}\")\n",
    "    for i in tqdm(range(num_epochs)):  \n",
    "        print(f\"========= Epoch {i} Training =========\")\n",
    "        # train steps\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            # x, y\n",
    "            data_map, label=data\n",
    "            data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "            label_int = label.long()\n",
    "            data_map_reshaped=data_map_reshaped.to(device)\n",
    "            label_int=label_int.to(device)\n",
    "            del data_map, label\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # y_pred\n",
    "            label_pred = model(data_map_reshaped)\n",
    "            # Loss Computation and Optimization\n",
    "            loss = criterion(label_pred,label_int)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # draw tensorboard\n",
    "            total_train_step = total_train_step + 1\n",
    "            # print info\n",
    "            if total_train_step % 1000 == 0:\n",
    "                end_time = time.time()\n",
    "                print(label_pred)\n",
    "                print(f\"Train time: {end_time - start_time}\")\n",
    "                print(f\"Train steps: {total_train_step}, Loss: {loss.item()}\")\n",
    "            writer_train.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "            # Clear gpu\n",
    "            del data, data_map_reshaped, label_int, label_pred, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluation and save the best model\n",
    "        print(f\"========= Epoch {i} Testing =========\")\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        test_count = 0\n",
    "        total_test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                test_count = test_count + 1\n",
    "                # x, y\n",
    "                data_map, label=data\n",
    "                data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "                label_int = label.long()\n",
    "                data_map_reshaped = data_map_reshaped.to(device)\n",
    "                label_int = label_int.to(device)\n",
    "                del data_map, label\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # y_pred\n",
    "                label_pred_test = model(data_map_reshaped)\n",
    "                loss = criterion(label_pred_test,label_int)\n",
    "#                 print(label_pred_test)\n",
    "                # accuracy \n",
    "                total_test_acc = total_test_acc + ((label_pred_test.argmax(1)) == label_int).sum()\n",
    "                # draw tensorboad\n",
    "                total_test_loss = total_test_loss + loss\n",
    "                if test_count % 100 == 0:\n",
    "                    print(f\"Loss: {total_test_loss} Accuracy: {total_test_acc/test_count}\")\n",
    "                # Clear gpu\n",
    "                del data_map_reshaped, label_int, label_pred_test, loss, data\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"Total Loss: {total_test_loss} Total Accuracy: {total_test_acc/test_count}\")\n",
    "        writer_valid.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "        writer_valid_acc.add_scalar(\"test_acc\", total_test_acc/test_count, total_test_step)\n",
    "        total_test_step = total_test_step + 1\n",
    "        print(\"..........Saving the model..........\")\n",
    "        torch.save(model.state_dict(),f\"../model/{classification}/{exper_dir}/Fold{fold}_Epoch{i}.pt\") \n",
    "#         if total_test_loss < min_test_loss:\n",
    "#             min_test_loss = total_test_loss\n",
    "#             print(\"..........Saving the model..........\")\n",
    "#             torch.save(model.state_dict(),f\"../model/{exper_dir}/Fold{fold}_Epoch{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30fa7482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5b6bf3977462>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "del train_data, train_label, test_data, test_label\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d621c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([2])\n",
      "tensor([1, 1])\n"
     ]
    }
   ],
   "source": [
    "test = torch.tensor([1])\n",
    "print(test.size())\n",
    "test = torch.tensor([test.item(),test.item()])\n",
    "print(test.size())\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "322cc097",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_map_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-25223a449931>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mdata_map_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mlabel_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_map_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Clear gpu\n",
    "del optimizer\n",
    "del scheduler\n",
    "del criterion\n",
    "del model\n",
    "del data_map_pred\n",
    "del label\n",
    "del label_pred\n",
    "del label_pred_test\n",
    "del loss\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "528060df",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
