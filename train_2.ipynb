{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2229f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from load_data import MyData  # self-made\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm # View procedure\n",
    "import os\n",
    "import scipy.io\n",
    "from random import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from net_cnn_lstm1 import MyNetwork\n",
    "from torchnlp.word_to_vector import GloVe\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c775b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "C,H,W = 1,1,2400\n",
    "learn_rate = 0.0005\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7fcb643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "manualSeed = 32\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41c40bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "device = torch.device(\"cuda:0\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ==损失函数权重\n",
    "# ======== 二分类HC/DOC\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 887 + 985 + 879\n",
    "# condition2\n",
    "# total_samples = 929 + 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 887 + 975 + 879\n",
    "# rest\n",
    "# total_samples = 852 + 1051 + 872\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 887, total_samples / (985 + 879)]\n",
    "# condition2\n",
    "# weights = [total_samples / 929, total_samples / (1029 + 886)]\n",
    "# condition3\n",
    "# weights = [total_samples / 852, total_samples / (1051 + 872)]\n",
    "\n",
    "# ======== 二分类MCS/UWS\n",
    "# 计算总样本数量\n",
    "# condition1\n",
    "# total_samples = 985 + 879\n",
    "# condition2\n",
    "# total_samples = 1029 + 886\n",
    "# condition3\n",
    "# total_samples = 975 + 879\n",
    "# rest\n",
    "total_samples = 1051 + 872\n",
    "# 计算每个类别的权重\n",
    "# condition1\n",
    "# weights = [total_samples / 985, total_samples / 879]\n",
    "# condition2\n",
    "# weights = [total_samples / 1029, total_samples / 886]\n",
    "# condition3\n",
    "# weights = [total_samples / 975, total_samples / 879]\n",
    "# condition3\n",
    "weights = [total_samples / 1051, total_samples / 872]\n",
    "# 将权重转换为张量\n",
    "weights_tensor = torch.tensor(weights, device=device)\n",
    "\n",
    "# 定义交叉熵损失函数并设置权重\n",
    "criterion = nn.CrossEntropyLoss(weight=weights_tensor)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f32739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:97: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2210645adf994da3bc99cbc64220d7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/eegmap_split/rest\\train\\mcs\\rest_mcs_train_0.pt\n",
      "../data/eegmap_split/rest\\train\\mcs\\rest_mcs_train_0_mcs_uws_label.pt\n",
      "../data/eegmap_split/rest\\test\\mcs\\rest_mcs_test_0.pt\n",
      "../data/eegmap_split/rest\\test\\mcs\\rest_mcs_test_0_mcs_uws_label.pt\n",
      "../data/eegmap_split/rest\\train\\uws\\rest_uws_train_0.pt\n",
      "../data/eegmap_split/rest\\train\\uws\\rest_uws_train_0_mcs_uws_label.pt\n",
      "../data/eegmap_split/rest\\test\\uws\\rest_uws_test_0.pt\n",
      "../data/eegmap_split/rest\\test\\uws\\rest_uws_test_0_mcs_uws_label.pt\n",
      "torch.Size([1539, 2400, 10, 11])\n",
      "torch.Size([1539])\n",
      "torch.Size([384, 2400, 10, 11])\n",
      "torch.Size([384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\pytorch\\lib\\site-packages\\ipykernel_launcher.py:160: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6797de77c0654470a22f29617d49ef55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= Epoch 0 Training =========\n",
      "tensor([[0.1114, 0.0473]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 150.3211076259613\n",
      "Train steps: 1000, Loss: 0.7257242798805237\n",
      "========= Epoch 0 Testing =========\n",
      "Loss: 68.5621337890625 Accuracy: 0.5999999642372131\n",
      "Loss: 137.24661254882812 Accuracy: 0.5999999642372131\n",
      "Loss: 207.0938262939453 Accuracy: 0.5533333420753479\n",
      "Total Loss: 265.16082763671875 Total Accuracy: 0.546875\n",
      "..........Saving the model..........\n",
      "========= Epoch 1 Training =========\n",
      "tensor([[ 0.2073, -0.0966]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 355.16181230545044\n",
      "Train steps: 2000, Loss: 0.8565868139266968\n",
      "tensor([[ 0.2669, -0.1023]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 516.0022947788239\n",
      "Train steps: 3000, Loss: 0.8947053551673889\n",
      "========= Epoch 1 Testing =========\n",
      "Loss: 68.74504089355469 Accuracy: 0.5399999618530273\n",
      "Loss: 138.17320251464844 Accuracy: 0.5349999666213989\n",
      "Loss: 206.09628295898438 Accuracy: 0.5699999928474426\n",
      "Total Loss: 263.6269836425781 Total Accuracy: 0.578125\n",
      "..........Saving the model..........\n",
      "========= Epoch 2 Training =========\n",
      "tensor([[ 0.2451, -0.0902]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 716.4972836971283\n",
      "Train steps: 4000, Loss: 0.8747748136520386\n",
      "========= Epoch 2 Testing =========\n",
      "Loss: 69.36917114257812 Accuracy: 0.5099999904632568\n",
      "Loss: 138.20570373535156 Accuracy: 0.5349999666213989\n",
      "Loss: 206.59561157226562 Accuracy: 0.5366666913032532\n",
      "Total Loss: 263.76910400390625 Total Accuracy: 0.5494791865348816\n",
      "..........Saving the model..........\n",
      "========= Epoch 3 Training =========\n",
      "tensor([[ 0.2660, -0.1336]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 916.3986446857452\n",
      "Train steps: 5000, Loss: 0.9127975702285767\n",
      "tensor([[-0.0498,  0.2601]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1071.2950661182404\n",
      "Train steps: 6000, Loss: 0.5501705408096313\n",
      "========= Epoch 3 Testing =========\n",
      "Loss: 65.88967895507812 Accuracy: 0.550000011920929\n",
      "Loss: 131.75450134277344 Accuracy: 0.5699999928474426\n",
      "Loss: 198.84329223632812 Accuracy: 0.5833333730697632\n",
      "Total Loss: 253.4855194091797 Total Accuracy: 0.609375\n",
      "..........Saving the model..........\n",
      "========= Epoch 4 Training =========\n",
      "tensor([[ 0.4484, -0.3698]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1269.8479764461517\n",
      "Train steps: 7000, Loss: 0.3654903471469879\n",
      "========= Epoch 4 Testing =========\n",
      "Loss: 63.811439514160156 Accuracy: 0.5899999737739563\n",
      "Loss: 129.00897216796875 Accuracy: 0.6100000143051147\n",
      "Loss: 190.19544982910156 Accuracy: 0.6166666746139526\n",
      "Total Loss: 242.89382934570312 Total Accuracy: 0.609375\n",
      "..........Saving the model..........\n",
      "========= Epoch 5 Training =========\n",
      "tensor([[ 0.9104, -0.8587]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1470.511156797409\n",
      "Train steps: 8000, Loss: 0.15741108357906342\n",
      "tensor([[-0.1674,  0.3971]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1622.4736766815186\n",
      "Train steps: 9000, Loss: 0.4502202272415161\n",
      "========= Epoch 5 Testing =========\n",
      "Loss: 67.1241683959961 Accuracy: 0.5699999928474426\n",
      "Loss: 129.069580078125 Accuracy: 0.6349999904632568\n",
      "Loss: 184.42767333984375 Accuracy: 0.6766666769981384\n",
      "Total Loss: 241.56011962890625 Total Accuracy: 0.6692708730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 6 Training =========\n",
      "tensor([[-0.3402,  0.5816]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 1823.211020708084\n",
      "Train steps: 10000, Loss: 0.3348848819732666\n",
      "========= Epoch 6 Testing =========\n",
      "Loss: 61.76594543457031 Accuracy: 0.6699999570846558\n",
      "Loss: 115.5790786743164 Accuracy: 0.6899999976158142\n",
      "Loss: 172.9434814453125 Accuracy: 0.6800000071525574\n",
      "Total Loss: 230.60891723632812 Total Accuracy: 0.6692708730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 7 Training =========\n",
      "tensor([[ 1.5388, -1.6809]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2024.2400770187378\n",
      "Train steps: 11000, Loss: 0.0391886942088604\n",
      "tensor([[ 1.8300, -1.9568]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2179.201873779297\n",
      "Train steps: 12000, Loss: 0.022416068241000175\n",
      "========= Epoch 7 Testing =========\n",
      "Loss: 79.38018798828125 Accuracy: 0.6800000071525574\n",
      "Loss: 157.96731567382812 Accuracy: 0.699999988079071\n",
      "Loss: 249.9975128173828 Accuracy: 0.6833333373069763\n",
      "Total Loss: 324.78643798828125 Total Accuracy: 0.6536458730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 8 Training =========\n",
      "tensor([[ 1.9006, -1.9522]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2381.0285563468933\n",
      "Train steps: 13000, Loss: 0.020996559411287308\n",
      "========= Epoch 8 Testing =========\n",
      "Loss: 77.42064666748047 Accuracy: 0.6399999856948853\n",
      "Loss: 147.04078674316406 Accuracy: 0.6699999570846558\n",
      "Loss: 216.1404266357422 Accuracy: 0.6800000071525574\n",
      "Total Loss: 290.1244201660156 Total Accuracy: 0.671875\n",
      "..........Saving the model..........\n",
      "========= Epoch 9 Training =========\n",
      "tensor([[ 1.5111, -1.6125]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2577.4802939891815\n",
      "Train steps: 14000, Loss: 0.04305803403258324\n",
      "tensor([[ 2.0691, -2.1791]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2732.754661798477\n",
      "Train steps: 15000, Loss: 0.014189540408551693\n",
      "========= Epoch 9 Testing =========\n",
      "Loss: 110.2070541381836 Accuracy: 0.5899999737739563\n",
      "Loss: 214.8510284423828 Accuracy: 0.6299999952316284\n",
      "Loss: 305.6330261230469 Accuracy: 0.6633333563804626\n",
      "Total Loss: 390.9006652832031 Total Accuracy: 0.6614583730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 10 Training =========\n",
      "tensor([[ 2.5643, -2.6327]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 2935.1601922512054\n",
      "Train steps: 16000, Loss: 0.005517966579645872\n",
      "========= Epoch 10 Testing =========\n",
      "Loss: 103.445068359375 Accuracy: 0.699999988079071\n",
      "Loss: 198.3499755859375 Accuracy: 0.7149999737739563\n",
      "Loss: 334.11505126953125 Accuracy: 0.6833333373069763\n",
      "Total Loss: 428.9749450683594 Total Accuracy: 0.6770833730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 11 Training =========\n",
      "tensor([[-2.4095,  2.8729]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3136.563437461853\n",
      "Train steps: 17000, Loss: 0.005067362450063229\n",
      "tensor([[-2.5080,  2.8917]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Train time: 3233.632976293564\n",
      "Train steps: 18000, Loss: 0.004507856443524361\n",
      "========= Epoch 11 Testing =========\n",
      "Loss: 118.14530944824219 Accuracy: 0.6599999666213989\n",
      "Loss: 241.8046112060547 Accuracy: 0.6549999713897705\n",
      "Loss: 382.1412658691406 Accuracy: 0.6399999856948853\n",
      "Total Loss: 488.4214172363281 Total Accuracy: 0.6458333730697632\n",
      "..........Saving the model..........\n",
      "========= Epoch 12 Training =========\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(dataset,mode):\n",
    "    # find the fold file\n",
    "    count = 0\n",
    "    for person in range(len(dataset)):\n",
    "        filename = os.path.join(dataset.path, dataset.file_path[person])\n",
    "        # extract the pure name of the file\n",
    "        parts = filename.split(\"\\\\\")\n",
    "        file_name = parts[-1]\n",
    "        name_without_extension = file_name.split(\".\")[0]\n",
    "        # label or data\n",
    "        file_last = name_without_extension.split(\"_\")[-1]\n",
    "        if file_last.isdigit(): # data\n",
    "            # is this fold or not\n",
    "            if int(file_last) == fold: # yes\n",
    "                print(filename)\n",
    "                count = count + 1\n",
    "                data_map = torch.load(filename)\n",
    "                # train or test\n",
    "                if name_without_extension.split(\"_\")[-2] == \"train\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        train_data.append(data_map[i])\n",
    "                elif name_without_extension.split(\"_\")[-2] == \"test\":\n",
    "                    for i in range(data_map.size(0)):\n",
    "                        test_data.append(data_map[i])\n",
    "                if count == 4:\n",
    "                    del data_map\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache() \n",
    "                    break\n",
    "            else:   # not\n",
    "                pass\n",
    "        else: # label\n",
    "            file_last = name_without_extension.split(\"_\")[-4]\n",
    "            file_mode = name_without_extension.split(\"_\")[-2]\n",
    "            if file_mode == mode.split(\"_\")[-1]: # is this mode or not\n",
    "                # is this fold or not\n",
    "                if int(file_last) == fold: # yes\n",
    "                    print(filename)\n",
    "                    count = count + 1\n",
    "                    data_map = torch.load(filename)\n",
    "                    # train or test\n",
    "                    if name_without_extension.split(\"_\")[-5] == \"train\":\n",
    "                        for i in range(data_map.size(0)):\n",
    "                            train_label.append(data_map[i])\n",
    "                    elif name_without_extension.split(\"_\")[-5] == \"test\":\n",
    "                        for i in range(data_map.size(0)):\n",
    "                            test_label.append(data_map[i])\n",
    "                    if count == 4:\n",
    "                        del data_map\n",
    "                        gc.collect()\n",
    "                        torch.cuda.empty_cache() \n",
    "                        break\n",
    "                else:   # not\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        del filename, parts, file_name, name_without_extension, file_last\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    \n",
    "# 定义LSTM超参数\n",
    "input_size = 64  # 输入特征维度\n",
    "hidden_size = 64  # 隐藏单元数量\n",
    "num_layers = 2  # LSTM层数\n",
    "output_size = 2  # 输出类别数量\n",
    "# 创建模型实例\n",
    "# model_list = ['', '_CNN', '_CNN_spa', '_CNN_spa_lstm']\n",
    "# model_name = model_list[0]\n",
    "# if model_name == model_list[0]: #  CascadeCept\n",
    "#     from network_cnn_lstm import MyNetwork\n",
    "# elif model_name == model_list[1]: #  CNN\n",
    "#     from network_cnn_lstm_2 import MyNetwork\n",
    "# elif model_name == model_list[2]: # CascadeCept_1\n",
    "#     from network_cnn_lstm_3 import MyNetwork\n",
    "# elif model_name == model_list[3]: # CascadeCept_2\n",
    "#     from network_cnn_lstm_4 import MyNetwork\n",
    "model_list = ['_1', '_2', '_3', '_4']\n",
    "# set model\n",
    "model_name = model_list[3]\n",
    "if model_name == model_list[0]: \n",
    "    from net_cnn_lstm1 import MyNetwork\n",
    "elif model_name == model_list[1]: \n",
    "    from net_cnn_lstm2 import MyNetwork\n",
    "elif model_name == model_list[2]: \n",
    "    from net_cnn_lstm3 import MyNetwork\n",
    "elif model_name == model_list[3]: \n",
    "    from net_cnn_lstm4 import MyNetwork\n",
    "model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "model = model.to(device)\n",
    "\n",
    "# experimental dir: rest, conditionA, conditionB, conditionC\n",
    "exper_dir = \"rest\"\n",
    "root_dir = f\"../data/eegmap_split/{exper_dir}\"\n",
    "# classification = \"hc_doc\"/doc or \"mcs_uws\"/uws\n",
    "# classification = \"hc_doc\"\n",
    "classification = \"mcs_uws\"\n",
    "fold_num = 5\n",
    "for fold in tqdm(range(5)):\n",
    "    # train num folds\n",
    "#     fold = 0 # 选择折数\n",
    "    # -- prepare datasets\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    \n",
    "    if classification == \"hc_doc\":\n",
    "        # ---- hc\n",
    "        dataset = MyData(root_dir, \"train\", \"hc\") # hc\n",
    "        make_dataset(dataset,classification)\n",
    "        dataset = MyData(root_dir, \"test\", \"hc\") # hc\n",
    "        make_dataset(dataset,classification)\n",
    "    # ---- mcs\n",
    "    dataset = MyData(root_dir, \"train\", \"mcs\") # mcs\n",
    "    make_dataset(dataset,classification)\n",
    "    dataset = MyData(root_dir, \"test\", \"mcs\") # hc\n",
    "    make_dataset(dataset,classification)\n",
    "    # ---- uws\n",
    "    dataset = MyData(root_dir, \"train\", \"uws\") # uws\n",
    "    make_dataset(dataset,classification)\n",
    "    dataset = MyData(root_dir, \"test\", \"uws\") # hc\n",
    "    make_dataset(dataset,classification)\n",
    "    \n",
    "    print(torch.stack(train_data).size())\n",
    "    print(torch.stack(train_label).size())\n",
    "    print(torch.stack(test_data).size())\n",
    "    print(torch.stack(test_label).size())\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()  \n",
    "    \n",
    "    train_data = torch.stack(train_data)\n",
    "    train_label = torch.stack(train_label)\n",
    "    test_data = torch.stack(test_data)\n",
    "    test_label = torch.stack(test_label)\n",
    "    # train dataset\n",
    "    train_td = TensorDataset(train_data, train_label)\n",
    "    train_loader = DataLoader(train_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    # test dataset\n",
    "    test_td = TensorDataset(test_data, test_label)\n",
    "    test_loader = DataLoader(test_td, batch_size = BATCH_SIZE, shuffle = True)\n",
    "    del train_data, train_label, test_data, test_label, train_td, test_td\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # set model for each fold\n",
    "    model = MyNetwork(input_size, hidden_size, num_layers, output_size)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 30, 50], gamma=0.2)\n",
    "    # -- start training\n",
    "    start_time = time.time()\n",
    "    # train and test step records\n",
    "    total_train_step = 0\n",
    "    total_test_step = 0\n",
    "    min_test_loss = 1000\n",
    "    # add Tensorboard\n",
    "    writer_train = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_train_{fold}\")\n",
    "    writer_valid = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_test_{fold}\")\n",
    "    writer_valid_acc = SummaryWriter(f\"../logs/{classification}/{exper_dir}{model_name}/logs_test_acc_{fold}\")\n",
    "    for i in tqdm(range(num_epochs)):  \n",
    "        print(f\"========= Epoch {i} Training =========\")\n",
    "        # train steps\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            # x, y\n",
    "            data_map, label=data\n",
    "            data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "            label_int = label.long()\n",
    "            data_map_reshaped=data_map_reshaped.to(device)\n",
    "            label_int=label_int.to(device)\n",
    "            del data_map, label\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            # y_pred\n",
    "            label_pred = model(data_map_reshaped)\n",
    "            # Loss Computation and Optimization\n",
    "            loss = criterion(label_pred,label_int)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # draw tensorboard\n",
    "            total_train_step = total_train_step + 1\n",
    "            # print info\n",
    "            if total_train_step % 1000 == 0:\n",
    "                end_time = time.time()\n",
    "                print(label_pred)\n",
    "                print(f\"Train time: {end_time - start_time}\")\n",
    "                print(f\"Train steps: {total_train_step}, Loss: {loss.item()}\")\n",
    "            writer_train.add_scalar(\"train_loss\",loss.item(),total_train_step)\n",
    "            # Clear gpu\n",
    "            del data, data_map_reshaped, label_int, label_pred, loss\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Evaluation and save the best model\n",
    "        print(f\"========= Epoch {i} Testing =========\")\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        test_count = 0\n",
    "        total_test_acc = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                test_count = test_count + 1\n",
    "                # x, y\n",
    "                data_map, label=data\n",
    "                data_map_reshaped = torch.reshape(data_map, (110, 1, 1, 2400))\n",
    "                label_int = label.long()\n",
    "                data_map_reshaped = data_map_reshaped.to(device)\n",
    "                label_int = label_int.to(device)\n",
    "                del data_map, label\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "                # y_pred\n",
    "                label_pred_test = model(data_map_reshaped)\n",
    "                loss = criterion(label_pred_test,label_int)\n",
    "#                 print(label_pred_test)\n",
    "                # accuracy \n",
    "                total_test_acc = total_test_acc + ((label_pred_test.argmax(1)) == label_int).sum()\n",
    "                # draw tensorboad\n",
    "                total_test_loss = total_test_loss + loss\n",
    "                if test_count % 100 == 0:\n",
    "                    print(f\"Loss: {total_test_loss} Accuracy: {total_test_acc/test_count}\")\n",
    "                # Clear gpu\n",
    "                del data_map_reshaped, label_int, label_pred_test, loss, data\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "        print(f\"Total Loss: {total_test_loss} Total Accuracy: {total_test_acc/test_count}\")\n",
    "        writer_valid.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n",
    "        writer_valid_acc.add_scalar(\"test_acc\", total_test_acc/test_count, total_test_step)\n",
    "        total_test_step = total_test_step + 1\n",
    "        print(\"..........Saving the model..........\")\n",
    "        torch.save(model.state_dict(),f\"../models/{classification}/{exper_dir}{model_name}/Fold{fold}_Epoch{i}.pt\") \n",
    "#         if total_test_loss < min_test_loss:\n",
    "#             min_test_loss = total_test_loss\n",
    "#             print(\"..........Saving the model..........\")\n",
    "#             torch.save(model.state_dict(),f\"../model/{exper_dir}/Fold{fold}_Epoch{i}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099a8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
